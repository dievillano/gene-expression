# Methods

## Experimental design and data {#sec-data}

```{r}
#| echo: false
labels <- c(
  "S1, S2, S3", "S4, S5, S6", "S7, S8, S9", "S10, S11, S12", "S13, S14, S15",
  "S16, S17, S18", "S19, S20, S21", "S22, S23, S24", "S25, S26, S27", 
  "S28, S29, S30"
)
conditions <- c("Y_30", "Y", "YC", "YE", "YCE")
strains <- c("WT", "FL")

label_matrix <- matrix(
  labels, nrow = 2, byrow = TRUE, dimnames = list(strains, conditions)
)

df <- tibble::as_tibble(label_matrix, rownames = "strain")
```

```{r}
#| echo: false
library(gt)
```

```{r}
#| echo: false
#| label: "tbl-01"
#| tbl-cap: "Overview of the experimental design showing the combinations of temperature and growth media (environmental conditions) used to grow cells of the WT and FLC1Δ (FL) strains. Each cell lists the replicate sample labels (S1–S30) corresponding to each unique experimental condition."
df |>
  gt() |>
  tab_spanner(label = md("**37°C**"), columns = c("Y", "YC", "YE", "YCE")) |>
  tab_spanner(label = md("**30°C**"), columns = "Y_30") |>
  tab_spanner(label = md("**Enviromental conditions**"), columns = -strain) |>
  cols_label(
    strain = md("**Strain**"),
    Y_30 = md("**Y**"),
    Y = md("**Y**"),
    YC = md("**YC**"),
    YE = md("**YE**"),
    YCE = md("**YCE**")
  ) |> 
  cols_align(align = "center") |> 
  tab_options(table.font.size = 10, table.width = "80%", table.align = "center")
```

```{r}
#| echo: false
counts_raw <- readr::read_delim(
  "data/GSE292021_counts_quantseq_CryptoRNAseqFLC1.txt", delim = "\t",
  col_types = paste0(c("ciiici", rep("i", 30)), collapse = "")
)
colnames(counts_raw)[1:6] <- c(
  "gene_id", "chr", "start", "end", "strand", "length"
)
```

The study involved culturing cells from both the wild-type (WT) and FLC1Δ mutant strains under various environmental conditions. Cells were grown in four different media: YPD (Y), YPD with CFW (YC), YPD with EGTA (YE), and YPD with both CFW and EGTA (YCE), all at 37°C to induce stress. Additionally, both strains were grown under baseline conditions in standard YPD at 30°C. In total, 10 distinct experimental conditions were tested (5 environmental conditions × 2 strains), each with 3 biological replicates, resulting in 30 samples. 

@tbl-01 presents the experimental design along with the sample labels for each condition. The 10 experimental conditions are labeled by combining the levels of the three experimental factors: strain, temperature, and growth media, in that order. We use the label FL to denote the FLC1Δ strain. For example, the condition involving the wild-type strain grown in YCE medium at 37°C is labeled WT-37-YCE, while the corresponding condition for the FLC1Δ strain is labeled FL-37-YCE.

The primary output of the experiment was a raw count matrix containing RNA abundance measurements for `r dim(counts_raw)[1]` genes across 30 samples, resulting in a `r dim(counts_raw)[1]` × 30 matrix. Each entry in the matrix represents the number of sequencing reads mapped to a specific gene in a given sample. 

## Pre-filtering {#sec-filter}

Some genes may exhibit low or zero counts across all samples, suggesting they are not expressed under the experimental conditions. These genes are unlikely to be identified as differentially expressed and are typically filtered out prior to analysis. The biological justification for pre-filtering low count or uninformative genes is that a gene typically needs to be expressed above a minimal threshold to be translated into a functional protein or to exert a meaningful biological effect [@chen2016from]. Moreover, low-expression genes often reflect sampling noise rather than true biological signal [@sha2015effect].

DE analysis involves performing a hypothesis test for each gene to evaluate whether expression levels differ between experimental conditions (see @sec-de). This results in a multiple testing problem, which is typically addressed by adjusting p-values to control the false discovery rate (FDR). However, such corrections reduce statistical power by raising the threshold for significance. As the number of tests increases, this loss of power aggravates, further limiting the ability to detect truly differentially expressed (DE) genes, especially when they represent a small proportion of the total set [@bourgon2010independent]. To mitigate this, it is recommended to filter out low-expression genes prior to DE analysis, thereby reducing the total number of hypotheses. This makes the multiple testing correction less stringent and increases the chance of correctly identifying DE genes [@bourgon2010independent; @sha2015effect].

Although the software used for differential expression (DE) analysis includes an internal filtering routine (see @sec-de), additional pre-filtering was applied using an empirical method implemented in the R package `edgeR` [@chen2025edgeR], as described in @chen2016from. This method retains genes whose counts-per-million (CPM) exceed a specified threshold $k$ in at least $n$ samples. The CPM for each gene in a sample is calculated by dividing the raw read count by the total number of reads (i.e., the library size) in that sample and scaling by one million.

To determine the CPM threshold $k$, the user specifies a minimum raw count that a gene must meet in $n$ samples, and the software computes the corresponding CPM based on the smallest library size. For this study, we required a minimum count of 10 in at least 3 samples, corresponding to the number of replicates per condition. The original, unfiltered count matrix was retained to replicate the DE analysis and compare results.

## Normalization and transformation of raw counts {#sec-norm}

In RNA-seq experiments, the number of reads mapped to a gene depends not only its expression level but also on between-sample and within-sample factors that make the row counts not directly comparable across genes or samples [@gierlinski2015statistical]. Between-sample variation primarily arises from differences in library size, with larger library sizes producing more reads across all genes in a sample, whereas within-sample variation occurs at a gene level and can be influenced by gene length or by the proportion of guanine (G) and cytosine (C) nucleotides in the genes [GC-content, @dillies2012a]. 

A common approach to correct for between-sample variation is total count normalization, in which raw counts are divided by the total number of reads (library size) for each sample and scaled by a constant factor. CPM is a typical example of this method. However, total count normalization can be problematic in DE analysis, as a small number of highly expressed genes can disproportionately inflate the library size. This can artificially lower the normalized expression of other genes within the same sample and exaggerate differences between samples, even when no real biological variation exists [@gierlinski2015statistical]. More robust normalization methods have been developed for DE analysis, such as the trimmed mean of M-values (TMM) method [@robinson2010scaling]. In this study, we employed the normalization procedure implemented in the R package `DESeq2` [@love2014moderated], which uses the median-of-ratios method to estimate sample-specific size factors for each gene count [@anders2010differential]. For comparison purposes, we also applied CPM normalization. No normalization to correct for within-sample variation (e.g., gene length or GC content) was applied, following the recommendation of the project advisor.

An additional challenge in RNA-seq analysis arises during data visualization and multivariate techniques such as clustering or principal component analysis (PCA), which are sensitive to the scale of the variables being analyzed. Even after normalization, RNA-seq data often exhibit heteroskedasticity, where genes with higher expression levels tend to show greater variance across samples than low-expressed genes. As a result, these highly variable genes can disproportionately influence the analysis, potentially obscuring meaningful biological patterns [@love2014moderated]. A common solution is to apply a variance-stabilizing transformation (VST) to the raw or normalized counts, which aims to place lowly and highly expressed genes on a common scale [@hafemeister2019normalization]. 

One of the most common VST is the logarithm (log) transformation. However, this transformation has the issue of exaggerating variability in low-count genes, where random noise tends to overshadow true biological signal [@love2014moderated]. In this study, we applied two transformations available in the `DESeq2` package: the regularized logarithm (rlog) transformation [@love2014moderated] and the VST proposed by @anders2010differential. The rlog transformation is closely related to the modeling framework used for differential expression analysis in `DESeq2` and will be described in more detail in @sec-de. The latter transformation, which we will refer to simply as VST, performs a monotonic transformation of the normalized counts so that the resulting variance is approximately independent of the mean. Both transformations were applied after normalizing the counts using the `DESeq2` method. To benchmark these transformations, we also applied a log transformation with base 2 to the CPM-normalized counts having previously added a prior count of 2 to the raw counts to avoid taking logarithm of zero (undefined).

## Exploratory analysis {#seq-eda}

An important first step in RNA-seq data analysis is assessing the quality and consistency of biological replicates. Ideally, replicates within the same experimental condition should exhibit similar expression profiles after normalization and transformation. To evaluate this, we computed both the correlation matrix and the pairwise distance matrix of the samples, using the Pearson correlation and the Euclidean distance, respectively. If the replicates are consistent, the correlation and distance matrices should reveal strong similarity among samples from the same condition and clear separation from those in different conditions. Prior to this analysis, the raw counts were normalized and transformed using the methods described in @sec-norm, resulting in three versions of the data: log-transformed CPM, rlog-transformed counts, and VST-transformed counts.

A more visual approach to assess replicate quality is to project the samples into a lower-dimensional space and evaluate whether replicates from the same condition cluster together. We used PCA for this purpose. PCA decomposes the correlation matrix of the genes into uncorrelated components ordered by the variance they explain, which is quantified by their corresponding eigenvalues. To determine how many dimensions to retain, we applied Horn’s parallel analysis: this method compares the observed eigenvalues to those obtained from randomly generated uncorrelated data sets and retains components whose eigenvalues exceed the average from the simulations [@dinno2009exploring]. 

We used biplots to visualise the samples alongside the five genes most strongly correlated with each component, for all pairwise combinations of the retained principal components. These correlations are quantified by the loadings. In addition, we generated loading plots displaying the top 1% of genes with the highest loading magnitudes for each retained component. To further investigate which experimental conditions or factors might be driving the separation of samples in PCA space, we produced boxplots of the sample scores (i.e., component coordinates) grouped by each experimental factor. This enabled us to identify potential groupings and assess which genes may be contributing to the observed patterns.

Prior to all PCA-related analyses, we selected the top 10% most variable genes across samples since these genes are more likely to capture biological signal, whereas low-variance genes typically contribute little or are uninformative for the purpose of visualizing the trends in the data. 

## Differential expression analysis {#sec-de}

The DE analysis approach used in this study follows the methodology implemented in the `DESeq2` package. The core idea is to model the raw counts for each gene using a negative binomial (NB) distribution, where the logarithm of the normalized mean is modeled as a linear combination of coefficients corresponding to contrasts of experimental conditions. Hypothesis testing is then performed on these coefficients to assess whether the corresponding contrasts result in statistically significant differential expression for a given gene. In the remainder of this section, we briefly outline the key features of this modeling framework, as described by @love2014moderated.

Let $c_{ij}$ denote the observed raw count for gene $i$ in sample $j$. We assume that $c_ij$ has a NB distribution with mean $\mu_{ij}$ and gene-specific dispersion parameter $\alpha_i$; that is, $c_{ij} \sim NB(\mu_{ij}, \alpha_i)$. The mean $\mu_{ij}$ is modelled as the product of a sample-specific size factor $s_j$ and a normalized expression level $q_{ij}$, such that $\mu_{ij} = s_j q_{ij}$. The size factor $s_j$ is estimated using the median-of-ratios method, as mentioned in @sec-norm. Finally, the dispersion parameter is used to model the variance of the counts via $\text{var}(c_{ij}) = \mu_{ij}+\alpha_i\mu_{ij}^2$.

The log-transformation of the normalized expression level, $q_{ij}$, is modelled in this study as

$$
\begin{aligned}
\text{log}(q_{ij}) &= \beta_{i0} + \beta_{i1}x^{(\text{FL})}_{j} + \beta_{i2}x^{(\text{YC})}_{j} + \beta_{i3}x^{(\text{YE})}_{j} + \beta_{i4}x^{(\text{YCE})}_{j} + \beta_{i5}x^{(\text{30})}_{j} + \\
& \quad \quad \beta_{i6}x^{(\text{FL})}_{j}x^{(\text{YC})}_{j} + \beta_{i7}x^{(\text{FL})}_{j}x^{(\text{YE})}_{j} + \beta_{i8}x^{(\text{FL})}_{j}x^{(\text{YCE})}_{j} + \beta_{i9}x^{(\text{FL})}_{j}x^{(\text{30})}_{j},
\end{aligned}
$$ {#eq-glm}

where the intercept term $\beta_{i0}$ denotes the baseline expression level of gene $i$ under the reference condition. The coefficients $\beta_{i1}$ to $\beta_{i5}$ represent the main effects of the experimental factors: $x^{(\text{FL})}_{j}$ is an indicator variable for the FLC1Δ strain, with the WT strain serving as the reference level; $x^{(\text{YC})}_{j}$, $x^{(\text{YE})}_{j}$, and $x^{(\text{YCE})}_{j}$ are indicators for the different media conditions, with Y as the reference level; and $x^{(\text{30})}_{j}$ indicates the 30°C temperature condition, with 37°C as reference. Together, this implies that the reference experimental condition is WT-37-Y. The interaction terms $\beta_{i6}$ to $\beta_{i9}$ model how the effects of media and temperature differ under the presence or absence of the FLC1 gene. No interaction between media and temperature was included, as these factors are not fully crossed in the experimental design.

Accurate estimation of the gene-specific dispersion parameters, $\alpha_i$, is crucial for robust DE analysis. However, in controlled experiments with small sample sizes (often only two or three replicates, as in this study) the maximum likelihood (ML) dispersion estimates can be highly variable, potentially compromising the reliability of the DE significance testing [@love2014moderated]. To address this, `DESeq2`  employs an an empirical Bayes shrinkage approach that dispersion estimates towards an mean expression-dependent trend. The procedure consists of the following steps:

1. Initial fitting: Estimate dispersion for each gene using the method of moments, then fit the model via MLE to obtain initial estimates $\hat{\mu}_{ij}^{(0)}$.
2. Gene-wise dispersion: Compute gene-wise dispersion estimates $\hat{\alpha}_i^{\text{GW}}$ by maximizing the Cox-Reid profile likelihood of $\alpha_i$ given $\hat{\mu}_{ij}^{(0)}$.
3. Trend estimation: Fit a parametric regression of $\hat{\alpha}_i^{\text{GW}}$ on the mean normalized counts $\bar{\mu}_i=\frac{1}{m}\sum_j{\frac{c_{ij}}{s_j}}$, where $m$ is the number of samples. This trend, $\bar{\alpha}_i$ models the expected dispersion as a function of the mean expression.
4. Shrinkage: Treat the trend values $\bar{\alpha}_i$ as prior means in a log-normal prior, $\log(\alpha_i) \sim \mathcal{N}(\log(\bar{\alpha}_i), \sigma^2)$. The prior variance $\sigma^2$ is estimated from the residual variance of log-dispersion values around the fitted trend. For experiments with fewer than three residual degrees of freedom (samples minus model parameters), $\sigma^2$ is estimated via simulation by matching the empirical distribution of residuals to simulated densities.
5. Final estimates: The final dispersion values are obtained as the maximum a posteriori (MAP) estimates, derived by combining the Cox-Reid profile log-likelihood with the log-normal prior distribution.

While this shrinkage approach performs well on average, it can underestimate the variance for genes with genuinely high dispersion, increasing the risk of false positives. To mitigate this, `DESeq2` uses the gene-wise dispersion estimate instead of the shrunken value when the former exceeds the fitted trend by more than two residual standard deviations.

The log-fold change (LFC) quantifies the change in gene expression between two experimental conditions and corresponds to the model coefficients in @eq-glm, typically expressed on a $\log_2$ scale. These coefficients are estimated via MLE, using the previously obtained shrunken dispersion estimates. However, MLE-derived LFC estimates can be highly variable, particularly for genes with low counts, making them noisy and potentially misleading. To enhance their stability and interpretability, `DESeq2` applies an empirical Bayes shrinkage procedure introduced by @zhu2019heavy and implemented in the `apeglm` package. This method proceeds through the following steps:

1. Prior specification: For each gene,a heavy-tailed Cauchy prior is assigned to the coefficients $\beta_{ik}$, for $k = 1, \ldots, p$, where $p$ is the number of coefficients excluding the intercept. The prior is defined as $\beta_{ik} \sim \text{Cauchy}(0, S_k)$, where 0 is the location parameter and $S_k$ is the scale parameter, which determines the amount of shrinkage. 
2. Scale estimation: $S_k$ is adaptively estimated from the data through an empirical Bayes approach. Specifically, that the MLEs $\hat{\beta}_{ik}$  follow a normal distribution around the true coefficient $\beta_{ik}$, with variance equal to their squared strandard errors $e_{ik}^2$. It further assumes that the true coefficients $\beta_{ik}$ are normally distributed with mean zero an unknown variance $A_k$. The empirical Bayes estimate $\hat{A}_k$ is used to derive the scale: $S_k = \sqrt{\hat{A}_k}$. If the MLEs are not provided, a default scale of $S_k=1$ is used. 
3. Final estimates: The final shrunken LFC estimates are obtained by combining the NB likelihood from the observed count data with the adaptive Cauchy prior. The result is the MAP estimate of $\beta_{ik}$.
4. Uncertainty quantification: A Laplace approximation is applied to the posterior distribution of $\beta_{ik}$ to approximate its variance. This provides an estimate of the posterior standard deviation, which can be used for statistical inference.

Shrinkage of LFC does not affect the number of genes identified as significantly differentially expressed, but improves the stability of estimates for downstream analyses such as visualization, gene filtering or functional analysis, where more reliable effect size estimates are needed @meeta2021hbctraining. 

The default approach in `DESeq2` is to perform a Wald test to assess whether a LFC is significantly different from zero, using the MLEs. In this study, the null hypothesis of no differential expression for gene $i$ between the reference condition and the condition indicated by $x_{ik}$ is $H_0: \beta_{ik} = 0$, with the alternative hypothesis being $\beta_{ik} \neq 0$. While the assumption of a zero LFC may be biologically implausible for many genes due to the high connectivity of gene regulatory networks, it serves as a useful baseline for statistical testing, particularly in small-scale studies such as this one with only three replicates per condition [@love2014moderated]. `DESeq2` adjusts p-values for multiple testing using the Benjamini–Hochberg procedure, which controls the false discovery rate (FDR) [@benjamini1995controlling]. Given the limited statistical power in our design due to few replicates per condition, we adopt an adjusted p-value threshold of 0.1 to determine significance.

Returning to the discussion of pre-filtering in @sec-filter, `DESeq2` applies independent filtering by default, using the mean of normalised counts across all samples as the filtering criterion. A threshold is automatically selected to maximise the number of discoveries at a target FDR, and genes with mean counts below this threshold are excluded from downstream analysis. This method is considered independent because the filter is uncorrelated with the test statistic under the null, a property shown to improve statistical power [@bourgon2010independent]. The adjusted p-value threshold for significance matches the target FDR, set to 0.1. `DESeq2` also includes automatic outlier detection to identify observations that disproportionately influence LFC estimates. An observation is flagged as an outlier if its Cook’s distance exceeds the 99th percentile of the $F(q, q-n)$ distribution, where $q$ is the number of model parameters and $n$ the number of samples. Outlier detection is skipped for conditions with two or fewer replicates. For six or fewer replicates, genes with outliers are excluded; with seven or more, outliers are replaced by imputed values (trimmed means), and the model is refitted.

The rlog transformation, introduced in @sec-norm, transforms the raw count $c_{ij}$ of gene $i$ at sample $j$ as $\text{rlog}(c_{ij})= log_2(q_{ij})=\beta_{i0}+\beta_{ik}$. It involves fitting a model similar to @eq-glm and computing shrunken LFCs relative to the baseline expression, $\beta_{i0}$, using an empirical Bayes shrinkage approach, as previously described for the full model. For this transformation, `DESeq2` uses blind dispersion estimation by default: it ignores the experimental design and treats all samples as replicates of the same condition when re-estimating dispersions. This makes the rlog-transformed data well-suited for unsupervised or exploratory analyses such as quality control, where influence from the experimental groups is undesirable. The VST transformation is also blinded by default.



